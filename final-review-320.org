#+title: CAS CS 320 Review
#+subtitle: Principles of Programming Languages@@html:<br>@@
#+subtitle: Boston University@@html:<br>@@
#+subtitle: Spring 2023
#+HTML_LINK_HOME: material.html
#+options: H:3 toc:2
This page contains an outline of the topics covered in /CAS CS 320:
Principles of Programming Languages/ during the Spring 2023 semester.
It is not exhaustive, and is biased towards those topics which are
most pertinent for the final exam of the course.  Included are
exercises associated with each topic.
* OCaml
During the first half of the course, we learned the function
programming language [[https://ocaml.org][OCaml]].  Our primary goal was to learn how to
program in the functional style, not thinking of a function as
defining a list of commands, but instead as specifying the /shape/ of
its output.
** Basics
OCaml has many types that are standard in most programming languages,
e.g., integers, Boolean values, floating-point numbers, tuples and
lists, all with basic operators.  It also has standard functional
language constructs like let-definitions/bindings, anonymous
functions, and if-else expressions.
*** Exercises
1. Implement the function
 #+begin_src ocaml
   let matrix_of_list
     (l : 'a list)
     (num_cols : int) : ('a list list) option =
     assert false (* TODO *)
 #+end_src
 which converts a list ~l~ into a matrix with ~num_cols~ columns,
 returing ~None~ in the case that ~num_cols~ is not positive or the
 resulting matrix is not rectangular (i.e., the length of ~l~ is not
 a multiple of ~num_cols~).
2. Euclid's algorithm for determining the greatest common divisor of
 two integers is takes advantage of the fact that $\mathsf{gcd}(m, n)
 = \mathsf{gcd}(n, m \bmod n)$.  Implement the function
 #+begin_src ocaml
   let gcd (m : int) (n : int) : int =
     assert false (* TODO *)
 #+end_src
 which, given two integers ~m~ and ~n~, returns their greatest
 commond divisor.
3. We represent a rational number as a pair of integers where the
   first integer represents the numerator and the second represents
   the denominator.  We maintain the invariant that the second number
   is positive and the pair of numbers are relatively prime.
   Implement the function ~add~ which adds two rational numbers,
   making sure to maintain this invariant.
   #+begin_src ocaml
     type rat = int * int

     let add (m : rat) (n : rat) : rat =
       assert false (* TODO *)
   #+end_src
4. Implement the function
   #+begin_src ocaml
     let gen_fib : (l : int list) (n : int) : int =
       if n < 0
       then assert false
       else
         assert false (* TODO *)
   #+end_src
   which, given a list of integers ~l~ of length $k$ and a nonnegative
   integer ~n~, returns the ~n~-th element of the following sequence:
   \begin{equation*}
   F_n =
   \begin{cases}
   l[n] & n < k \\
   \sum_{i = 1}^k F_{n - i} & n \geq k
   \end{cases}
   \end{equation*}
** Algebraic Data Types
One of the most important features of a modern functional programming
language is pattern matching and algebraic data types.  An algebraic
data type (ADT) is defined by giving a collection of *constructors*,
which may themselves carry other data.  Here, for example, is an ADT
whose values represent either a Boolean values or an integers.
#+begin_src ocaml
  type bool_or_int
    = Bool of bool
    | Int of int
#+end_src
We work with values of an ADT by *pattern matching*, providing
different values depending on the /shape/ of the given value.
#+begin_src ocaml
  let num_val (x : bool_or_int) : int =
    match x with
    | Bool b -> if b then 1 else 0
    | Int i -> i
#+end_src
ADTs can be /recursive/ (the type being defined can be referred to in
the definition of the type) and /parametric/ (the type being defined
can depend parametrically on another type).  These two features are encompassed in the ~list~ type.
#+begin_src ocaml
  type 'a mylist
    = Nil
    | Cons of 'a * 'a mylist
#+end_src
*** Exercises
** Record Types
Record types are essentially tuples with named fields.
#+begin_src ocaml
  type rat =
    { numer : int
    ; denom : int
    ; is_positive : bool
    }

  let two_thirds =
    { numer = 2
    ; denom = 3
    ; is_positive = true
    }
#+end_src
Beyond this, they have a couple conveniences that are good to remember.
+ Accessing fields in a record can be done via dot notation, i.e., if
  ~r~ is a ~rat~ then ~r.is_positive~ is a ~bool~.
+ Updating a small number of fields records can be done using ~with~-notation.
   #+begin_src ocaml
     let negate (r : rat) : rat =
       { r with is_positive = not r.is_positive }

     let recip (r : rat) : rat =
       if r.numer = 0
       then assert false
       else { r with numer = r.denom; denom = r.numer }
   #+end_src
*** Exercises
1. merge user data
2. add to captured bindings
3. list of records to dictionary

** Higher-Order Programming
Higher-order programming is the use of functions as first-class values
to write general, reusable code.  There are three patterns in
particular for higher-order programming with lists which we looked at
in depth.
+ The function ~map~, defined as
  #+begin_src ocaml
    let rec map (f : 'a -> 'b) (l : 'a list) : 'b list =
      match l with
      | [] -> []
      | x :: l -> let x = f x in x :: map f l
  #+end_src
  replaces each element in ~l~ with ~f~ applied to that element, in
  order from left to right.
+ The function ~filter~, defined as
  #+begin_src ocaml
    let rec filter (p : 'a -> bool) (l : 'a list) : 'a list =
      match l with
      | [] -> []
      | x :: l -> if p x then x :: filter p l else filter p l
  #+end_src
  find all element of ~l~ which satisfy the predicate ~p~, in order
  from left to right.
+ The function ~fold_right~, defined as
  #+begin_src ocaml
    let rec fold_right (f : 'a -> 'b -> 'b) (l : 'a list) (accu : 'b) : 'b =
      match l with
      | [] -> accu
      | x :: l -> f x (fold_right f l accu)
  #+end_src
  applies the binary operation ~f~ between every element of ~l @ [accu]~ right-associatively:
  #+begin_src
     [x1;   x2;   x3; ...   xn]
      ↓↓    ↓↓    ↓↓        ↓↓
    f x1 (f x2 (f x3 (...(f xn accu)...)))
  #+end_src
  The function ~fold_left~ does the same but to ~accu :: l~  left-associatively:
  #+begin_src ocaml
    let rec fold_left (f : 'b -> 'a -> 'b) (accu : 'b) (l : 'a list) : 'b =
      match l with
      | [] -> accu
      | x :: l -> fold_left f (f accu x) l
  #+end_src
  #+begin_src
                       [x1; x2; x3; ... xn]
                        ↓↓  ↓↓  ↓↓      ↓↓
    f (...(f (f (f accu x1) x2) x3)...) xn
  #+end_src
*** Exercises
1. predicate operations
2. find the derivative (newtons)
3. radix sort bucketing as fold_left
** Tail Recursion
Roughly speaking, a recursive call in the body of a function
definition is in *tail position* if no evaluation is required /after/
the recursive call.  The following implementation of the factorial
function is not tail recursive because it requires evaluating the
product of the result of its recursive call with the input ~n~.
#+begin_src ocaml
  let rec factorial (n : int) : int =
    if n < 0
    then assert false
    else if n = 0
    then 1
    else n * factorial n
#+end_src
We tend to make functions tail recursive by adding an accumulator argument to the function.
#+begin_src ocaml
  let factorial_tail (n : int) : int =
    let rec go (n : int) (accu : int) =
      if n = 0
      then accu
      else go (n - 1) (n * accu)
    in
    if n < 0
    then assert false
    else go n 1
#+end_src
*** Exercises
1. hour glass + tr hour glass
2. tr evaluator
** Type Checking
OCaml is strongly typed, and it is statically checked for adherence to
typing rules.  OCaml also has type inference, which means we often do
not have to specify the types of expressions in OCaml programs (though
it can be useful for documentation purposes).
*** Exercises
1. What is the type of this?
2. Does this type check?
* Formal Grammar
In general, grammar is the study of the form structure of language.
We use concepts from the study of formal grammar to represent and
reason about the syntax of programming languages.  These concepts
inform the design of parsers.
** BNF Specifications
We start with a collection of symbols, separated into to disjoint
groups, the *nonterminal* symbols and the *terminal* symbols.  In a
Backus-Naur Form (BNF) specification we use notation like ~<nonterm>~
to denote a nonterminal symbols.  We typically don't specify the
symbols in advance, but instead glean then from the specification
itself.

*** Definitions
A *sentential form* is a sequences of symbols and a *sentence* is a
sequence of terminal symbols.

A *production rule* is made up of a nonterminal symbol and a
sentential form, and is written
#+begin_src
  <nonterm> ::= SENTFORM
#+end_src
We interpret a production rule as indicating that ~<nonterm>~ /stands
for/ ~SENTFORM~ in a sentence.

A *BNF specification* is given by a collection of production rules and
a *starting symbol*.  We typically take the nonterminal symbol in the
/first/ rule of the specification to be the starting symbol.

A *derivation* of a sentential form $S$ in a specification $\mathcal
B$ with start symbol ~<start>~ is a sequence of sentential forms,
starting with the start symbol ~<start>~ and ending in $S$, in which
each form in the sequence (except for ~<start>~) is the results of
replacing a nonterminal symbol in the previous form with a sentential
from given by a production rule in $\mathcal B$.

A derivation is *leftmost* if the nonterminal symbol replaced at each
step of the derivation is the leftmost nonterminal symbol in the
sentential form.

A *parse tree*, informally, is a derivation represented as a tree, in
which the root is labeled with the starting symbol and each node
contains as it's children the symbols of sentential form which it is
replaced with in the derivation. A parse tree may correspond to
multiple derivations, but every derivation has a unique parse tree
representation.

*** Examples
TODO
*** Exercises
1. List the symbols implicit in this specification.
2. Give a derivation of S in this grammar.
3. Give a leftmost derivation of S in this grammar.
4. Give an example of a grammar with no leftmost derivations.
5. Draw the parse tree associated with this derivation.
6. Draw the parse tree for this sentence in this grammar.
7. Implement the algorithm which determines the sentence for a given
   parse_tree.
** Ambiguity
A BNF specification is *ambiguous* if there is a sentence with
multiple parse trees.  We try to avoid ambiguous specifications for
programming languages because we don't want a program to be
interpretable in multiple ways.

*** Fixity

The *fixity* of an operator refers to where the operator is written
with respect to its arguments.
+ *prefix* operators appear /before/ their argument
  + the negation operator: ~-5~
+ *postfix* operators appear /after/ their argument
  + type constructors: ~int list~
+ *infix* (binary) operators appear /between/ their arguments
  + arithmetic operators: ~(1 + 2) * (3 + 4)~)
+ *mixfix* operators are a combination of these
  + if-else-expressions: ~if not b the f x else g x~

One of the primary ways in which specifications of programming
languages can be ambiguous is in the use of infix and mixfix
operations.  If a language's syntactic constructs are all prefix
(Polish notation) or all postfix (reverse Polish notation) then the
specification is unambiguous.

We can make infix binary operators unambiguous by specifying their
associativity and precedence.
*** Associativity
An operator $\square$ is declared *left associative* if we interpret
$a \square b \square c$ to be equivalent to $(a \square b) \square c$.
+ For arithmetic expressions, we take subtraction to be
  left-associative, so the expression ~1 - 2 - 3~ evaluates to ~-4~
  as opposed to ~2~.

An operator $\square$ is declared *right associative* if we interpret
$a \square b \square c$ to be equivalent to $a \square (b \square c)$.
+ For arithmetic expressions, we take exponentiation to be
  right-associative, so the expression ~2 ^ 1 ^ 3~ evaluates to ~2~
  as opposed to ~8~.

We can enforce the associativity of an operator in the specification itself.
*** Precedence
Given two binary operators $\square$ and $\triangle$, the operator
$\square$ has *higher precedence* than $\triangle$ if we interpret $a
\square b \triangle c$ as $(a \square b) \triangle c$ and $a \triangle
b \square c$ as $a \triangle (b \square c)$.
+ For arithmetic expressions, we take multiplication to have higher
  precedence than addition, so the expression ~2 * 2 + 3~ evaluates to
  ~7~ as opposed to ~10~.

As with associativity, we can enforce precedence within the
specification itself.

TODO
*** Exercises
1. Is this grammar ambiguous?
2. Find a sentence in this grammar which has multiple derivations.
3. Update this grammar so that it makes the operations precedence and hold
** Regular Grammars
Regular grammars are a special form of grammar.

Regular expressions are a compact representation of regular grammars.
** Chomsky Norm Form
TODO
* Parsing
The general parsing problem is to find a derivation of a sentence, if
one exists.  (NOTE ON CHOMSKY)

In the context of this course, we are more interested in the specific
problem of converting a string representation of a program into an
algebraic data type representing the syntax of the program.

There are many ways to accomplish this, we saw two.

** Recursive-Descent

Recursive-descent parsing refers to an ad-hoc form of parsing in which
mutually recursive functions are defined to parse forms of a
specification.

This is best understood by example.

TODO

(Note: It is unlikely that recursive-descent parsing will appear on
the final exam in any significant way.)

** Combinators

We can think of a parser for ~'a~'s as a functions of type
#+begin_src ocaml
  type 'a parser = char list -> ('a * char list) option
#+end_src
which
1. consumes the prefix of the input stream corresponding to an ~'a~,
2. converts that prefix to an ~'a~, and finally,
3. returns that ~'a~ and the remainder of the stream, failing if no
   initial part of the stream corresponds to an ~'a~.

One of the simplest examples is the ~char~ parser:
#+begin_src ocaml
  let char (d : char) (cs : char list) : (char * char list) option =
    match cs with
    | c :: cs when c = d -> Some (d, cs)
    | _ -> None
#+end_src
which consumes the first character of ~cs~ given that it is equal to
~d~ and returns it, along with the remainder of ~cs~.  This parser
fails (returns ~None~) in the case that the first character of ~cs~ is
not ~d~.

When we want to /use/ a parser, we apply it to a character list and
verify that it consumed it's the entire input:
#+begin_src ocaml
  let parse (p : 'a parser) (s : string) =
    match p (explode s) with
    | (a, []) -> Some a
    | _ -> None
#+end_src

A *parser combinator* is a higher-order function which can be used to
compose parsers. There is a small subset of parser combinators which
are of particular importance because they correspond to the constructs
in EBNF specifications.
+ *Alternatives.* ~p1 <|> p2~ is the parser which tries running the
  parser ~p1~, returning its output if it succeeds, and running ~p2~
  otherwise.

  If ~p1~ is a parser for the forms of a nonterminal symbol ~<p1>~
  and ~p2~ a parser for forms of a nonterminal symbol ~<p2>~, then ~p1
  <|> p2~ is a parser for forms of the nonterminal symbol
  #+begin_src
    <alt> ::= <p1> | <p2>
  #+end_src
+ *Sequencing.* ~seq p1 p2~ is the parser which runs both ~p1~ and
  ~p2~ and returns both of their outputs if both parsers succeed.  It
  fails if either ~p1~ or ~p2~ fails.

  If ~p1~ is a parser for the forms of a nonterminal symbol ~<p1>~ and
  ~p2~ a parser for forms of a nonterminal symbol ~<p2>~, then ~p1 <|>
  p2~ is a parser for forms of the nonterminal symbol
  #+begin_src
    <seq> ::= <p1> <p2>
  #+end_src
+ *Repetition.* ~many p~ is the parser which runs ~p~ repeatedly until
  it fails, collecting all its outputs in a list.

  If ~p~ is a parser for the forms of a nonterminal symbol ~<p>~ and
  ~p2~ a parser for forms of a nonterminal symbol ~<p2>~, then ~many
  p~ is a parser for forms of the nonterminal symbol
  #+begin_src
    <many> ::= { <p> }
  #+end_src

The last important combinator is ~map~, which can be used to
manipulate the output of a parser without affecting how it consumes
its input.  If ~p~ is an ~'a parser~, and ~f~ is a function of type
~'a -> 'b~, then ~map f p~ is a ~'b parser~ which runs ~'a~ and then
applies ~f~ to its output (if it succeeds).

You should also be familiar with how to use the more convenient parser
combinators throughout the course (though, for the exam, you will not
be required to memorize their definitions).
+ ~str~, ~token~, ~ws~
+ (~>>~), (~<<~), ~seq3~, ~seq4~
+ (~>|~), ~map2~, ~map3~, ~map4~
+ ~pure~, ~fail~, (we will not test on ~bind~ (~>>=~) but it is good to know...)


*** Exercises
* Formal Semantics
In general, semantics is the study of the /meaning/ of language.  We
use concepts from the study of formal semantics to model the behavior
(i.e., the meaning) of programs.

We discussed two forms of semantics, *denotational* semantics and
*operational* semantics.

Giving a denotational semantics for a programming language means
assigning to each a /mathematical function/ which has the same
input/output behavior as the program.

In this course, we focus on operational semantics.  Giving an
operational semantics for a programming language means describing how
a program in the language is evaluated.  This typically means defining
a *reduction relation* on programs, which describes how a program is
reduced until it reaches a state which cannot be further reduced.

** Derivations

Suppose we have a programming language $\mathcal P$ defined by a given BNF grammar.

A *configuration* is a pair consistent of a program $P$ and a state
$S$ which may be manipulated by programs.  The state may be empty, as
in the case of functional languages.

Defining the (small-step) operational semantics for $\mathcal P$ means
defining a reduction relation for configurations:
\begin{equation*}
( \ S \ , \ P \ ) \longrightarrow ( \ S' \ , \ P' \ )
\end{equation*}

A reduction relation is typically defined via *reduction rules*, which
consist of a /shape/ of a reduction together with a collection of
*premises*, which may be shapes of reductions or trivial premises
(also called axioms). The general form of a reduction rules is
something like
\begin{equation*}
\frac
{P_1 \qquad P_2 \qquad \dots \qquad P_k}
{C \longrightarrow C'}
\ \textsf{(name)}
\end{equation*}

/Shape/ here refers to the fact that the configurations in a reduction
rule contains /meta-variables/ that describe the /kind of reductions/
that can be derived, or that can be used a premises.  For example, the
rules for evaluating an arithmetic expression might include
\begin{equation*}
\frac{e_1 \longrightarrow e_1'}
{e_1 + e_2 \longrightarrow e_1' + e_2}
\ \textsf{(add-left)}
\end{equation*}
which expresses that if $e_1$ reduces to $e_1'$ in a single step then
$e_1 + e_2$ reduces to $e_1 + e_2'$ in a single step /no matter the
expressions $e_1$, $e_1'$ and $e_2$/.  This rule can be used to show that
\begin{equation*}
(1 + 2) + 3 \longrightarrow 3 + 3
\end{equation*}
but also that
\begin{equation*}
(1 + (2 * 3)) + 3 \longrightarrow (1 + 6) + 3
\end{equation*}
Both reductions invoke that fact that if the left argument can be
reduced by a single step, then the sum can be reduced by a single
step.

A *derivation* is, informally, a tree in which each node is a
reduction and the children of a node are the premises required to
derive that conclusion.  The leaves of a derivation are trivial
premises (axioms).

A derivation of a reduction $C \longrightarrow C'$ is a derivation
whose root is $C \longrightarrow C'$.

TODO derivation

It is generally preferable that any derivable reduction has a unique
derivation.  This defining an evaluation procedure easier, and amounts
to fixing an *evaluation order*.  We can often enforce an evaluation
order via the structure of our reduction rules.

What we have been describing is a *single-step reduction relation*.
Any single-step reduction relation can be extended to a *multi-step
reduction relation* by including the following rules and a multi-step reduction relation symbol
'$\longrightarrow^{\star}$'.
\begin{equation*}
\frac
{}
{C \longrightarrow^\star C}
\ \textsf{(refl)}
\qquad
\frac
{C \longrightarrow^\star C' \qquad C' \longrightarrow C''}
{C \longrightarrow^\star C''}
\ \textsf{(trans)}
\end{equation*}

Formally, *evaluation* is the process of determining a configuration $C'$ for a
given configuration $C$ such that $C \longrightarrow^\star C'$ and
$C'$ cannot be further reduced (i.e., there is no other configuration
$C''$ such that $C' \longrightarrow C''$).

*** Exercises
1. Write a derivation for this reduction.
2. Find a configuration which this single-step reduces to.
3. Write a derivation for this multi-step reduction.
4. Find a configuration for this multi-step reduction.
5. Show that this reduction relation is not normalizing.
6. Show that this relation is not unique.
7. Make this reduction update the above relation so that it has unique
   reductions and arguments are evaluated from left to right.
** Examples
*** Arithmetic Expressions
*** The Lambda Calculus
*** A Stack-Oriented Language
One of the running examples we used for understanding operations
semantics was stack-oriented languages.

The following is a BNF specification for a stack-oriented language
whose programs manipulate a stack of integers.
#+begin_src
  <prog>  ::= { <com> }
  <com>   ::= <num> | drop | dup | swap | .
            | + | - | *
            | then <com> else <com> end
  <num>   ::= <digit> { <digit> }
  <digit> ::= 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9
#+end_src

Here, for example, is a program which determines the prints the value of $14^2 + 15^2$.
#+begin_src
  14 dup *
  15 dup *
  + .
#+end_src
In the operational semantics of this language we take a configuration
to be a program ($P$) together with a stack of integers ($S$) and a trace of
strings ($T$) that are printed throughout the evaluation of the program.
\begin{equation*}
( \ S \ , \ T \ , \ P \ )
\end{equation*}
with a special error configuration
\begin{equation*}
\mathsf{ERROR} \equiv ( \ \varnothing \ , \ \textsf{panic} :: \varnothing \ , \ \epsilon \ )
\end{equation*}

The operational semantics then describe how each command affects the stack and the trace.

TODO
*** Exercises
TODO
* Subroutines
** Parameter Passing
** Examples
*** Lambda Calculus with Call-by-Name Operational Semantics
*** Lambda Calculus with Call-by-Value Operational Semantics
*** Stack-Oriented Language with Subroutines
* Environments
We need to specify how those variable bindings are represented in a
configuration.

When introducing variables to a programming language, we have to
specify when/where those bindings are accessible. This is called the
*scope* of the binding.  In general, this is a complex question, but
is broad strokes there are two paradigms: *dynamic scoping* and
*lexical scoping*.
** Stack-Oriented Language with Variables
* Dynamic Scoping
Dynamic scoping refers to the idea of using /computational (temporal)
context/ to determine when a binding is available.  In its simplest
form, we may think of all bindings as globally available as soon as
they have been instantiated.

In a language with dynamic scoping and /subroutines/, when it comes to
determining what bindings are available, it doesn't matter where a
subroutine is defined, but rather where it is called.

Bash is a widely used language with dynamic scoping, but in general,
dynamic scoping is not common in modern programming languages.  It is,
however, much easier to implement than lexical scoping.
** Stack-Oriented Language with Dynamic Scoping
* Lexical Scoping
** Activation Records
*** Stack-Oriented Language with Mutable Variables and Lexical Scoping
** Closures
*** Stack-Oriented Language with Immutable Variables and Lexical Scoping
* Compilation
* Solutions to Exercises
